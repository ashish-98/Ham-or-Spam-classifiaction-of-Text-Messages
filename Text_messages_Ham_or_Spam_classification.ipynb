{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Quiz 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWDB3Huac-Vl"
      },
      "source": [
        "**HAM OR SPAM CLASSIFICATION OF TEXT MESSAGES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ7yNMAHWdiX"
      },
      "source": [
        "import requests\n",
        "\n",
        "###Importing the messsage dataset\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                \n",
        "if __name__ == \"__main__\":\n",
        "    file_id = '1e_Azf9zGvSWsDhM9PP2sfMNKC72-iWAK'\n",
        "    destination = 'data.txt'\n",
        "    download_file_from_google_drive(file_id, destination)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utUNx7twKWRP"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MplBG7oMdBpT"
      },
      "source": [
        "with open('data.txt', 'r') as f:\n",
        "  data_raw = f.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYHiOvjtdcz2"
      },
      "source": [
        "def first_five_in_list(data_raw):\n",
        "  l_5 = []\n",
        "  if(len(data_raw)>5):\n",
        "    for i in range(5):\n",
        "      l_5.append(data_raw[i])\n",
        "  else:\n",
        "    l_5 = []    \n",
        "  return l_5\n",
        "\n",
        "\n",
        "first_five_in_list(data_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnU8WJSHgO80"
      },
      "source": [
        "def remove_trailing_newlines(s): \n",
        "  s_clean = s.replace(\"\\n\",\"\")\n",
        "  return s_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh7mGIS2BdaK"
      },
      "source": [
        "def mapl(f,l):\n",
        "  f_l = []\n",
        "  for i in range(len(l)):\n",
        "    f_l.append(f(l[i]))\n",
        "\n",
        "  return f_l\n",
        "\n",
        "mapl(remove_trailing_newlines,data_raw) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGMNHYDpSTza"
      },
      "source": [
        "def split_at_s(text, s):\n",
        "  a = text.split(s)\n",
        "  split_text = (a[0],a[1])\n",
        "  return split_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tpIVA2BP5dZ"
      },
      "source": [
        "def split_at_tab(text):\n",
        "  b = split_at_s(text,'\\t')\n",
        "  return b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNd46m-nQh3o"
      },
      "source": [
        "data_clean2 = []\n",
        "for line in data_clean:\n",
        "  data_clean2.append(split_at_tab(line))\n",
        "print(data_clean2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRiMxZhHkCLt"
      },
      "source": [
        "import string\n",
        "def remove_punctuations_and_lower(text):\n",
        "  return (text.translate(str.maketrans(\"\",\"\", string.punctuation))).lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgf3DUvqlmAI"
      },
      "source": [
        "data_set1 = []\n",
        "def rem_pun():\n",
        "  for i in data_clean2:\n",
        "    y = remove_punctuations_and_lower(i[1])\n",
        "    data_set1.append((i[0],y))\n",
        "  return data_set1\n",
        "rem_pun()  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE6n5CopEzsF"
      },
      "source": [
        "count_dict = {}\n",
        "def counter(l, f):\n",
        "  count1 = 0\n",
        "  count2 = 0\n",
        "  count_dict = {}\n",
        "  a = []\n",
        "  a = f(l)\n",
        "  for item in a:\n",
        "    if(item=='ham'):\n",
        "      count1 += 1\n",
        "    elif(item=='spam'):\n",
        "      count2 += 1\n",
        "  count_dict = {'ham':count1,'spam':count2}    \n",
        "\n",
        "  return count_dict\n",
        "counter(data_set1,aux_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN78MkCkTXwS"
      },
      "source": [
        "  def aux_func(l):\n",
        "    m = []\n",
        "    for i in range(len(l)):\n",
        "      m.append(l[i][0])\n",
        "    return m  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thveIifRLIIY"
      },
      "source": [
        "import random\n",
        "def random_shuffle(l):\n",
        "  l_shuffled = []  \n",
        "  random.shuffle(l)\n",
        "  for i in range(len(l)):\n",
        "    l_shuffled.append(l[i])\n",
        "  return l_shuffled\n",
        "random_shuffle(data_set1)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhosY1teXB6z"
      },
      "source": [
        "import math\n",
        "size = len(data_set1)\n",
        "a =math.floor(len(data_set1)*0.8)\n",
        "print(a)\n",
        "data_train = data_set1[0:a]\n",
        "data_test = data_set1[a:size]\n",
        "print(len(data_test))\n",
        "print(data_train)\n",
        "print(data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yOIc2n6qGXQ"
      },
      "source": [
        "vocab = []\n",
        "for tuple_a in data_train:\n",
        "  for words in tuple_a[1].split(\" \"):\n",
        "    if words not in vocab:\n",
        "      vocab.append(words)\n",
        "print(vocab)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfccWLOMuNSD"
      },
      "source": [
        "dict_spam = {}\n",
        "dict_ham = {}\n",
        "count = 0\n",
        "count2 = 0\n",
        "for tuple_a in data_train:\n",
        "  for word in tuple_a[1].split(\" \"):\n",
        "    if tuple_a[0] == 'ham':\n",
        "      if word in dict_ham:\n",
        "        dict_ham[word] += 1\n",
        "        count += 1\n",
        "      else:\n",
        "        dict_ham[word] = 1\n",
        "        count += 1\n",
        "    elif tuple_a[0] == 'spam':\n",
        "      if word in dict_spam:\n",
        "        dict_spam[word] += 1\n",
        "        count2 += 1\n",
        "      else:\n",
        "        dict_spam[word] = 1\n",
        "        count2 += 1\n",
        "print(dict_ham)\n",
        "print(dict_spam)    \n",
        "print(count)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOpCfpbPvx38"
      },
      "source": [
        "dict_prob_spam = {}\n",
        "dict_prob_ham = {}\n",
        "a = len(vocab)\n",
        "for word in vocab:\n",
        "  if word in dict_ham:\n",
        "    prob_ham = (dict_ham[word]+1)/(count+a)\n",
        "  else:\n",
        "    prob_ham = 0  \n",
        "  dict_prob_ham[word] = prob_ham  \n",
        "  if word in dict_spam:\n",
        "    prob_spam = (dict_spam[word]+1)/(count2+a)  \n",
        "  else:\n",
        "    prob_spam = 0  \n",
        "  dict_prob_spam[word] = prob_spam  \n",
        "print(dict_prob_ham)  \n",
        "print(dict_prob_spam)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJI6h4rw07z"
      },
      "source": [
        "def predict(text, dict_prob_spam, dict_prob_ham, data_train):\n",
        "  count_ham = 0\n",
        "  count_spam = 0\n",
        "  ham_score = 0\n",
        "  spam_score = 0\n",
        "  a = len(data_train)\n",
        "  for tuple_a in data_train:\n",
        "    if(tuple_a[0]=='ham'):\n",
        "      count_ham += 1\n",
        "    elif(tuple_a[0]=='spam'):\n",
        "      count_spam += 1  \n",
        "  p_spam = count_spam/a\n",
        "  p_ham = count_ham/a\n",
        "  for word in text.split(\" \"):\n",
        "    if word in dict_prob_spam.keys():\n",
        "      spam_score = p_spam * dict_prob_spam[word]\n",
        "    if word in dict_prob_ham.keys():\n",
        "       ham_score = p_ham * dict_prob_ham[word]\n",
        "  if ham_score < spam_score:\n",
        "    prediction = 'spam'\n",
        "  else:\n",
        "    prediction = 'ham'\n",
        "  return prediction     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCt0GIO3zuM1"
      },
      "source": [
        "def accuracy(data_test, dict_prob_spam, dict_prob_ham, data_train):\n",
        "  accuracy = 0\n",
        "  correct_pred = 0\n",
        "  for tuple_a in data_test:\n",
        "    prediction = predict(tuple_a[1], dict_prob_spam, dict_prob_ham, data_train)\n",
        "    if tuple_a[0] == prediction:\n",
        "      correct_pred +=1\n",
        "  accuracy = correct_pred / len(data_test)\n",
        "  return accuracy\n",
        "accuracy(data_test,dict_prob_spam,dict_prob_ham,data_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}